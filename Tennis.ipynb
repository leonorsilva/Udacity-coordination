{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, -0.009999999776482582]\n",
      "Score (max over agents) from episode 1: 0.0\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.10000000149011612, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.10000000149011612]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[0.0, 0.0]\n",
      "[-0.009999999776482582, 0.0]\n",
      "Score (max over agents) from episode 2: 0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 3):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        print(rewards)\n",
    "        \n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm1d(state_size)\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.fc4 = nn.Linear(32, action_size)\n",
    "\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        if state.dim() == 1:\n",
    "            state = torch.unsqueeze(state,0)\n",
    "        x = self.bn0(state)\n",
    "        x = F.selu(self.bn1(self.fc1(x)))\n",
    "        x = F.selu(self.bn2(self.fc2(x)))\n",
    "        x = F.selu(self.bn3(self.fc3(x)))\n",
    "        return torch.tanh(self.fc4(x))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm1d(state_size)\n",
    "        self.fcs1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128 + action_size, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.fc5 = nn.Linear(16, 1)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        if state.dim() == 1:\n",
    "            state = torch.unsqueeze(state,0)\n",
    "        state = self.bn0(state)\n",
    "        x_state = F.selu(self.fcs1(state))\n",
    "        x = torch.cat((x_state, action), dim=1)\n",
    "        x = F.selu(self.fc2(x))\n",
    "        x = F.selu(self.fc3(x))\n",
    "        x = F.selu(self.fc4(x))\n",
    "        return  F.selu(self.fc5(x))   #torch.tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0.00     # L2 weight decay\n",
    "BATCH_SIZE = 128         # minibatch size  128\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "        \n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "        \n",
    "        self.scheduler_Actor = torch.optim.lr_scheduler.StepLR(self.actor_optimizer,step_size=50, gamma=0.1)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "        self.scheduler_Critic = torch.optim.lr_scheduler.StepLR(self.critic_optimizer,step_size=50, gamma=0.1)\n",
    "\n",
    "        \n",
    "        self.deep_copy(self.actor_target, self.actor_local)\n",
    "        self.deep_copy(self.critic_target, self.critic_local)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "        \n",
    "        self.actor_losses=[]\n",
    "        self.critic_losses=[]\n",
    "        \n",
    "        self.epsilon=1\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.actor_target.eval()\n",
    "        self.critic_target.eval()\n",
    "        self.critic_local.eval()\n",
    "        \n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        statee = torch.from_numpy(state).float().to(device).unsqueeze(0)\n",
    "        next_statee = torch.from_numpy(next_state).float().to(device).unsqueeze(0)\n",
    "        actionn = torch.from_numpy(action).float().to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_next = self.actor_target(next_statee).cpu().data.numpy()\n",
    "            action_next = torch.from_numpy(action_next).float().to(device)\n",
    "            Q_targets_next = self.critic_target(next_statee, action_next).squeeze(0).detach().numpy()[0]\n",
    "            Q_targets = reward/0.1 + (GAMMA * Q_targets_next * (1 - int(done)))\n",
    "            Q_expected = self.critic_local(statee, actionn).squeeze(0).detach().numpy()[0]\n",
    "        error=abs(abs(Q_expected)-abs(Q_targets))\n",
    "        \n",
    "        self.memory.push(error, state, action, reward, next_state, done)\n",
    "        self.actor_target.train()\n",
    "        self.critic_target.train()\n",
    "        self.critic_local.train()\n",
    "        \n",
    "        \n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device).unsqueeze(0)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "            #return np.clip(action, -1, 1)\n",
    "            if np.random.uniform(0, high=1) < self.epsilon:\n",
    "                action= np.array([np.random.uniform(-1, high=1,size=2)])\n",
    "                \n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def start_learn(self):\n",
    "        if self.memory.__len__() > BATCH_SIZE:\n",
    "            self.learn( GAMMA)\n",
    "        \n",
    "    def learn(self, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones, idxs, is_weights= self.memory.sample(BATCH_SIZE)\n",
    "                \n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        actions = torch.from_numpy(actions).float().to(device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(device)\n",
    "        next_states = torch.from_numpy(next_states).float().to(device)\n",
    "        dones = torch.from_numpy(dones.astype(np.uint8)).float().to(device)\n",
    "\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = (torch.FloatTensor(is_weights)*F.mse_loss(Q_expected, Q_targets)).mean()\n",
    "        #print(rewards.detach().numpy().reshape(1,-1))\n",
    "        #print(Q_expected.detach().numpy().reshape(1,-1))\n",
    "        #print(Q_targets.detach().numpy().reshape(1,-1))\n",
    "   \n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "        #self.scheduler_Critic.step()\n",
    "        \n",
    "        errors=abs(abs(Q_expected.detach())-abs(Q_targets.detach()))\n",
    "        print(min(errors),max(errors))\n",
    "        self.memory.update_priorities(idxs, errors)\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -(self.critic_local(states, actions_pred)).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        #self.scheduler_Actor.step()\n",
    "\n",
    "        self.actor_losses.append(actor_loss.detach())\n",
    "        self.critic_losses.append(critic_loss.detach())\n",
    "            \n",
    "                   \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "    def deep_copy(self, target, source):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):  #0.4\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([np.random.randn() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity,bootstrap=4, prob_alpha=0.3):\n",
    "        self.prob_alpha = prob_alpha\n",
    "        self.capacity   = capacity\n",
    "        self.beta       = 1\n",
    "        self.buffer     = np.empty((0,5), int)\n",
    "        self.pos        = 0\n",
    "        self.priorities = np.array([])\n",
    "        self.bootstrap=bootstrap\n",
    "    \n",
    "    def push(self,prio, state, action, reward, next_state, done):\n",
    "        assert state.ndim == next_state.ndim\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "        \n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer=np.append(self.buffer,[[state, action, reward, next_state, done]],axis=0)\n",
    "            self.priorities=np.append(self.priorities,prio+0.1)\n",
    "        else:\n",
    "            self.buffer=np.delete(self.buffer,np.s_[:100],0)\n",
    "            self.priorities=np.delete(self.priorities,np.s_[:100])\n",
    "            self.buffer=np.append(self.buffer,[[state, action, reward, next_state, done]],axis=0)\n",
    "            self.priorities=np.append(self.priorities,prio+0.1)\n",
    "        \n",
    "       \n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:len(self.buffer)]\n",
    "        \n",
    "        probs  = np.power(prios,self.prob_alpha)\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs,replace=False)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        total    = len(self.buffer)\n",
    "        weights  = np.power(total * probs[indices], -self.beta)\n",
    "        weights /= weights.max()\n",
    "        weights  = np.array(weights, dtype=np.float32)\n",
    "        \n",
    "        batch       = np.array(samples).transpose()\n",
    "        states = np.vstack(batch[0])\n",
    "        actions = np.vstack(batch[1])\n",
    "        rewards = np.vstack(batch[2]/0.1)\n",
    "        next_states = np.vstack(batch[3])\n",
    "        dones =np.vstack(batch[4])\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "    \n",
    "    \n",
    "    def sample_bootstrap(self, batch_size):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities[:-self.bootstrap]\n",
    "        else:\n",
    "            prios = self.priorities[:len(self.buffer)-self.bootstrap]\n",
    "        \n",
    "        probs  = np.power(prios,self.prob_alpha)\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer)-self.bootstrap, batch_size, p=probs,replace=False)\n",
    "        \n",
    "        total    = len(self.buffer)\n",
    "        weights  = np.power(total * probs[indices], -self.beta)\n",
    "        weights /= weights.max()\n",
    "        weights  = np.array(weights, dtype=np.float32)\n",
    "        \n",
    "        new_indices=[list(range(i,i+self.bootstrap)) for i in indices]\n",
    "        new_indices=np.reshape(new_indices,(1,-1))[0]\n",
    "        \n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        batch       = np.array(samples).transpose()\n",
    "        states = np.vstack(batch[0])\n",
    "        actions = np.vstack(batch[1])\n",
    "        \n",
    "        samples = [self.buffer[idx] for idx in new_indices]\n",
    "        batch       = np.array(samples).transpose()\n",
    "        rewards = batch[2]/0.1\n",
    "        next_states = batch[3]\n",
    "        dones =batch[4]\n",
    "        final=[]\n",
    "        next_states_final=[]\n",
    "        dones_final=[]\n",
    "        aux=0\n",
    "        for i in range(0, len(rewards), self.bootstrap):\n",
    "            aux=0\n",
    "            for j in range(0,self.bootstrap):\n",
    "                aux+=rewards[i+j]\n",
    "                if dones[j]:\n",
    "                    break\n",
    "            next_states_final.append(next_states[j])\n",
    "            dones_final.append(dones[j])\n",
    "            final.append(aux)\n",
    "        rewards=np.vstack(np.array(final))\n",
    "        dones_final=np.vstack(np.array(dones_final))\n",
    "        #print(len(next_states_final))\n",
    "        #print(next_states_final)\n",
    "        next_states_final=np.vstack(np.array(next_states_final))\n",
    "        \n",
    "        return states, actions, rewards, next_states_final, dones_final, indices, weights\n",
    "    \n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio + 0.1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0.00     # L2 weight decay\n",
    "BATCH_SIZE = 128         # minibatch size  128\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "        \n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "        \n",
    "        self.scheduler_Actor = torch.optim.lr_scheduler.StepLR(self.actor_optimizer,step_size=50, gamma=0.1)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "        self.scheduler_Critic = torch.optim.lr_scheduler.StepLR(self.critic_optimizer,step_size=50, gamma=0.1)\n",
    "\n",
    "        \n",
    "        self.deep_copy(self.actor_target, self.actor_local)\n",
    "        self.deep_copy(self.critic_target, self.critic_local)\n",
    "        self.num_agents=2\n",
    "        # Noise process\n",
    "        self.noise = OUNoise((self.num_agents, action_size), random_seed)\n",
    "        \n",
    "        self.actor_losses=[]\n",
    "        self.critic_losses=[]\n",
    "        \n",
    "        self.epsilon=1\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.actor_target.eval()\n",
    "        self.critic_target.eval()\n",
    "        self.critic_local.eval()\n",
    "        \n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        statee = torch.from_numpy(state).float().to(device).unsqueeze(0)\n",
    "        next_statee = torch.from_numpy(next_state).float().to(device).unsqueeze(0)\n",
    "        actionn = torch.from_numpy(action).float().to(device).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_next = self.actor_target(next_statee).cpu().data.numpy()\n",
    "            action_next = torch.from_numpy(action_next).float().to(device)\n",
    "            Q_targets_next = self.critic_target(next_statee, action_next).squeeze(0).detach().numpy()[0]\n",
    "            Q_targets = reward/0.1 + (GAMMA * Q_targets_next * (1 - int(done)))\n",
    "            Q_expected = self.critic_local(statee, actionn).squeeze(0).detach().numpy()[0]\n",
    "        error=abs(abs(Q_expected)-abs(Q_targets))\n",
    "        \n",
    "        self.memory.push(error, state, action, reward, next_state, done)\n",
    "        self.actor_target.train()\n",
    "        self.critic_target.train()\n",
    "        self.critic_local.train()\n",
    "        \n",
    "        \n",
    "    def act(self, states, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        actions = np.zeros((self.num_agents, self.action_size))\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            for agent_num, state in enumerate(states):\n",
    "                action = self.actor_local(state).cpu().data.numpy()\n",
    "                actions[agent_num, :] = action\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            actions += self.noise.sample()\n",
    "            #return np.clip(action, -1, 1)\n",
    "            if np.random.uniform(0, high=1) < self.epsilon:\n",
    "                actions= np.array([np.random.uniform(-1, high=1,size=2),np.random.uniform(-1, high=1,size=2)])\n",
    "                \n",
    "        return np.clip(actions, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def start_learn(self):\n",
    "        if self.memory.__len__() > BATCH_SIZE:\n",
    "            self.learn( GAMMA)\n",
    "        \n",
    "    def learn(self, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones, idxs, is_weights= self.memory.sample(BATCH_SIZE)\n",
    "                \n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        actions = torch.from_numpy(actions).float().to(device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(device)\n",
    "        next_states = torch.from_numpy(next_states).float().to(device)\n",
    "        dones = torch.from_numpy(dones.astype(np.uint8)).float().to(device)\n",
    "\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = (torch.FloatTensor(is_weights)*F.mse_loss(Q_expected, Q_targets)).mean()\n",
    "        #print(rewards.detach().numpy().reshape(1,-1))\n",
    "        #print(Q_expected.detach().numpy().reshape(1,-1))\n",
    "        #print(Q_targets.detach().numpy().reshape(1,-1))\n",
    "   \n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "        #self.scheduler_Critic.step()\n",
    "        \n",
    "        errors=abs(abs(Q_expected.detach())-abs(Q_targets.detach()))\n",
    "        self.memory.update_priorities(idxs, errors)\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -(self.critic_local(states, actions_pred)).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        #self.scheduler_Actor.step()\n",
    "\n",
    "        self.actor_losses.append(actor_loss.detach())\n",
    "        self.critic_losses.append(critic_loss.detach())\n",
    "            \n",
    "                   \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "    def deep_copy(self, target, source):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):  #0.4\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.size=size\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity,bootstrap=4, prob_alpha=0.4):\n",
    "        self.prob_alpha = prob_alpha\n",
    "        self.capacity   = capacity\n",
    "        self.beta       = 1\n",
    "        self.buffer     = np.empty((0,5), int)\n",
    "        self.pos        = 0\n",
    "        self.priorities = np.array([])\n",
    "        self.bootstrap=bootstrap\n",
    "    \n",
    "    def push(self,prio, state, action, reward, next_state, done):\n",
    "        assert state.ndim == next_state.ndim\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "        \n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer=np.append(self.buffer,[[state, action, reward, next_state, done]],axis=0)\n",
    "            self.priorities=np.append(self.priorities,prio+0.1)\n",
    "        else:\n",
    "            self.buffer=np.delete(self.buffer,np.s_[:100],0)\n",
    "            self.priorities=np.delete(self.priorities,np.s_[:100])\n",
    "            self.buffer=np.append(self.buffer,[[state, action, reward, next_state, done]],axis=0)\n",
    "            self.priorities=np.append(self.priorities,prio+0.1)\n",
    "        \n",
    "       \n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:len(self.buffer)]\n",
    "        \n",
    "        probs  = np.power(prios,self.prob_alpha)\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs,replace=False)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        total    = len(self.buffer)\n",
    "        weights  = np.power(total * probs[indices], -self.beta)\n",
    "        weights /= weights.max()\n",
    "        weights  = np.array(weights, dtype=np.float32)\n",
    "        \n",
    "        batch       = np.array(samples).transpose()\n",
    "        states = np.vstack(batch[0])\n",
    "        actions = np.vstack(batch[1])\n",
    "        rewards = np.vstack(batch[2]/0.1)\n",
    "        next_states = np.vstack(batch[3])\n",
    "        dones =np.vstack(batch[4])\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "    \n",
    "    \n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio + 0.1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent= Agent(state_size, action_size, random_seed=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 1\tAverage Score: 0.00\tScore: -0.005\tGreedy: 0.30\r",
      "Episode 2\tAverage Score: 0.00\tScore: -0.005\tGreedy: 0.30\r",
      "Episode 3\tAverage Score: 0.00\tScore: -0.005\tGreedy: 0.30\r",
      "Episode 4\tAverage Score: 0.00\tScore: -0.005\tGreedy: 0.30"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20205231\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\ipykernel_launcher.py:152: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50\tAverage Score: 0.01\tScore: 0.045\tGreedy: 0.300\n",
      "Episode 100\tAverage Score: 0.02\tScore: 0.045\tGreedy: 0.30\n",
      "Episode 150\tAverage Score: 0.02\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 200\tAverage Score: 0.01\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 250\tAverage Score: 0.01\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 300\tAverage Score: 0.01\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 350\tAverage Score: 0.01\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 400\tAverage Score: 0.00\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 450\tAverage Score: 0.00\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 500\tAverage Score: 0.00\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 550\tAverage Score: 0.00\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 600\tAverage Score: 0.00\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 650\tAverage Score: 0.01\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 700\tAverage Score: 0.02\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 750\tAverage Score: 0.02\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 800\tAverage Score: 0.03\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 850\tAverage Score: 0.03\tScore: 0.045\tGreedy: 0.300\n",
      "Episode 900\tAverage Score: 0.03\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 950\tAverage Score: 0.03\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 1000\tAverage Score: 0.02\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 1050\tAverage Score: 0.01\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 1100\tAverage Score: 0.01\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 1150\tAverage Score: 0.01\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 1200\tAverage Score: 0.02\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 1250\tAverage Score: 0.03\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 1300\tAverage Score: 0.03\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 1350\tAverage Score: 0.03\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 1400\tAverage Score: 0.03\tScore: 0.095\tGreedy: 0.300\n",
      "Episode 1450\tAverage Score: 0.03\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 1500\tAverage Score: 0.02\tScore: -0.005\tGreedy: 0.30\n",
      "Episode 1508\tAverage Score: 0.02\tScore: -0.005\tGreedy: 0.30"
     ]
    }
   ],
   "source": [
    "learn_every=1   #4\n",
    "num_learn=1   #1\n",
    "goal_score=0.5\n",
    "scores = []\n",
    "avg_score_list = []\n",
    "scores_deque = deque(maxlen=100)\n",
    "t=0\n",
    "for i_episode in range(1, 5000):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    score = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    agent.reset()\n",
    "\n",
    "    while True:\n",
    "        actions = agent.act(states)\n",
    "        env_info = env.step(actions)[brain_name]            # send all actions to the environment\n",
    "        next_states = env_info.vector_observations          # get next state (for each agent)\n",
    "        rewards = env_info.rewards                          # get reward (for each agent)\n",
    "        dones = env_info.local_done                         # see if episode finished\n",
    "        agent.step(states[0], actions[0], rewards[0],next_states[0], dones[0])\n",
    "        agent.step(states[1], actions[1], rewards[1],next_states[1], dones[1])\n",
    "        score += rewards                           # update the score (for each agent)\n",
    "        states = next_states                                # roll over states to next time step\n",
    "        \n",
    "        t+=1\n",
    "        if t%learn_every == 0:\n",
    "            t=0\n",
    "            if agent.memory.__len__() > BATCH_SIZE:\n",
    "\n",
    "                for _ in range(num_learn):\n",
    "                    agent.start_learn()\n",
    "                agent.soft_update(agent.critic_local, agent.critic_target, TAU)\n",
    "                agent.soft_update(agent.actor_local, agent.actor_target, TAU)\n",
    "\n",
    "        if np.any(dones):  # exit loop if episode finished\n",
    "            break\n",
    "    agent.epsilon=max(agent.epsilon*0.995,0)\n",
    "    agent.memory.prob_alpha=agent.memory.prob_alpha*0.995\n",
    "\n",
    "    scores_deque.append(np.max(score))\n",
    "    scores.append(np.max(score))\n",
    "    avg_score = np.mean(scores_deque)\n",
    "    avg_score_list.append(avg_score)\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.3f}\\tGreedy: {:.2f}'.\\\n",
    "          format(i_episode, avg_score, np.mean(score),agent.memory.prob_alpha), end=\"\")\n",
    "    if i_episode % 50 == 0 or avg_score > 0.5:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque))) \n",
    "\n",
    "        if avg_score > 0.5:\n",
    "            print('\\nEnvironment solved in {:d} episodes!'.format(i_episode))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array(scores)\n",
    "np.savetxt(\"foo.csv\", a, delimiter=\",\")\n",
    "a=np.array(avg_score_list)\n",
    "np.savetxt(\"foo2.csv\", a, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scripted = torch.jit.script(agent.actor_local) # Export to TorchScript\n",
    "model_scripted.save('actor_local.pt') # Save\n",
    "model_scripted = torch.jit.script(agent.critic_local) # Export to TorchScript\n",
    "model_scripted.save('critic_local.pt') # Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.plot(np.arange(1, len(avg_score_list)+1), avg_score_list)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size, random_seed=8)\n",
    "\n",
    "agent.actor_local = torch.jit.load('actor_local.pt')\n",
    "agent.critic_local = torch.jit.load('critic_local.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f1fdf9c470>,\n",
       " <matplotlib.lines.Line2D at 0x1f1fdf9c4e0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA7p0lEQVR4nO2deZwcVbXHv6e7p2eyh2wsIexBBdkji8t7gA8BUdHnxiIqi3lR8QHqQ3xuiAuCC8IDjGERAdkEBISQsENYgtnIShImK5N1QpLJ7NPddd8fvVV3V1VX91TP9HTO9/NJpurWrVv3Vnf/6tS5954rxhgURVGUgU+ovyugKIqiBIMKuqIoSo2ggq4oilIjqKAriqLUCCroiqIoNUKkvy48ZswYc8ABB/TX5RVFUQYk8+bN22aMGet0rN8E/YADDmDu3Ln9dXlFUZQBiYisczumLhdFUZQaQQVdURSlRlBBVxRFqRFU0BVFUWoEFXRFUZQaQQVdURSlRlBBVxRFqRFU0BVFGRg0zYONb/V3LaqafptYpCiKUhK3n5r8e3VL/9ajilELXVEUpUZQQVcURakRVNAVRVFqBBV0RVGUGkEFXVEUpUZQQVcURakRVNAVRVFqBBV0RVGUGkEFXVEUpUZQQVcURakRVNAVRVFqhKKCLiJ3ishWEVnicvx8EVmU+ve6iBwVfDUVRVGUYvix0O8CzvA4vgb4d2PMkcAvgGkB1EtRFEUpkaLRFo0xr4jIAR7HX7ftzgb2DaBeiqIoSokE7UO/GHja7aCITBaRuSIyt7m5OeBLK4qi7N4EJugicgpJQf+BWx5jzDRjzCRjzKSxY8cGdWlFURSFgBa4EJEjgduBM40x7wVRpqIoilIavbbQRWQ/4FHgAmPMyt5XSVEURSmHoha6iNwPnAyMEZEm4GdAHYAxZirwU2A0cKuIAMSNMZMqVWFFURTFGT+jXM4tcvwS4JLAaqQoiqKUhc4UVRRFqRFU0BVFUWoEFXRFUZQaQQVdURSlRlBBVxRFqRFU0BVFUWoEFXRFUZQaQQVdURSlRlBBVxSlulgxA7a+HVhxL63YyrKNuwIrz4l177UzffGmil7DD4EE51IURQmM+7+c/Ht1SyDFff0vcwBY+5uzAinPiU/c8Ardcaui1/CDWuiKoii9pDtu9XcVABV0RVGUmkEFXVEUpUZQQVcURakRVNAVRVFqBBV0RVGUGkEFXVEUpUZQQVcURakRVNAVRVFqBBV0RVGUGkEFXVEUpUYoKugicqeIbBWRJS7HRURuEpFGEVkkIscGX01FURSlGH4s9LuAMzyOnwlMTP2bDPyp99VSFEVRSqWooBtjXgG2e2Q5G7jbJJkNjBSRvYOqoKIoSjXy9qZdzFjS/yFz7QThQx8PvGvbb0qlFSAik0VkrojMbW5uDuDSiqIo/cOZN85iyr3z+7saOQQh6OKQZpwyGmOmGWMmGWMmjR07NoBLK4qieLO24TzurrsWFj8MV4+Ane8WP6lEfhm5g7UN5wVebqkEIehNwATb/r7AxgDKVRRFCYR/Cy+Gt/6W3GleEXj5X4k8H3iZ5RCEoD8BfDU12uVEoMUYU12OJUVRFJNahEKcnAq1QdEl6ETkfuBkYIyINAE/A+oAjDFTgenAJ4FGoAO4sFKVVRRFKRuT8gRL7U6/KSroxphzixw3wLcDq5GiKEolyFjotSvotdsyRVEUO7uBhV67LVMURbGjFrqiKEqNoIKuKIpSI+wGo1xU0BVF2T3oAwvdGMc5lX2GCrqiKLsH6nJRFEWpFdKjXNTloiiKMrCxEsm/FXW5VKxoX6igK4oSOAvf3cnTiz0igGxZBoseqsi1X31nG681bitIT1hJl4txjCdYG6igK4oSOGff8hrf/JtHaNk/nQSPfqMi1/7KHbM5//bZBembdnYAsHBDa0BXMghWXkr/UnTqv6IoykBiTv23SBACPpWTblKdonETjIV+e93v+I/wAqAlkPKCQAVdUZSaYqw4C6ykBF0C6hRNinlyqGK6xOSwxf5z6ajLRVGU3QJJOUSCFr3+7gi1o4KuKMpuQdZCD7Zc47LdH6igK4qyW5C20CVg2bWqyERXQVcUZbcgPSIlcAu9evRcBV1RlN0DSSlvKGBBt1voxtJYLoqiKBUn0ylau/OKVNAVRdk9yPrQgyXHQs+baNTXqKArirJbkPahhwJ2oqsPXVEUpY9J+9ArOcplQPjQReQMEVkhIo0icpXD8REi8k8RWSgiS0XkwuCrqihKrVPJBSJCFbLQrX4WcTtFBV1EwsAtwJnAYcC5InJYXrZvA8uMMUcBJwO/F5FowHVVFKXGqaT7ImuZB3wRy+43r34L/Xig0Riz2hjTAzwAnJ2XxwDDJBkkYSiwHYgHWlNFUfqH1S/B8un9XQtftHW7y07ah75s4y5mLNncq+vcM3tdZvvhues8cvYtfgR9PPCubb8plWbnZuADwEZgMXCZSYc2syEik0VkrojMbW5uLrPKiqL0KXefDQ+cW9IpISzCJEq+VG/t2+tnLHc9lrbQp72yiin3zuvVdX7y2JLM9h9mLM4eGABrijo5nPJrfTrwFrAPcDRws4gMLzjJmGnGmEnGmEljx44tsaqKogwUHoxew6qGC0o+r7c+9IO2veB6TCoktksbLs5s9/eIFz+C3gRMsO3vS9ISt3Mh8KhJ0gisAd4fTBUVRRlofCi0sl+ue3jr667HKjUOPZfqt9DnABNF5MBUR+c5wBN5edYDHwcQkT2B9wGrg6yooii1T1+McuktXnV08DT3KUUXuDDGxEXkUmAmEAbuNMYsFZEpqeNTgV8Ad4nIYpIPwB8YYwoX9VMURfGgkoIeVLTF/nareOFrxSJjzHRgel7aVNv2RuATwVZNURQlOEJ94g6pfpeLoihKn9B7l4W7hzwTPre3FrrXsQHQKaooitInVFIQg7LQPd1CA2DYoqIoSh/RW0F0Pz+oUS6eFnovy+4tKuiKolQPfdApWlnUQlcURQGC0HN3+zvUB6NcKjlKxw8q6IqiVA39vUCEH4zXA0EFXVEUpe/oG9dL/6CCrihK1dDbBSICDnXuiKfLRX3oiqIMVGIJi+tnLKe1K+Z4/NV3tvHUok2+yytVEHviFtd5XL83bN3VxR+fW1myX3x1cxu3vdI/kU9U0BVFKZt/zN/ArS+t4vfPOAfj+sodb/Lt++a7F9BLn/M/FjTx15eW8odn/QcDqyNBlOIPgEv+Oodpzy1m8YaWgmMNdDueYyzDF6e+wa+mv01XrPTwwb1FBV1RlLLpSVg5f0sl3/o1VmnlDN2+hGUNF3HItheT5/sYZf5g/S9Y2fA1zzxz127nhM33sazhIkJtW/LqDMsb3FfZ9Fpko9KooCuKUja99xgbj73ijGlJLjRxSNscILgOz3nrdnBWeDYA9e250cK9R7nYNvvBna6CrihK+Zjezb4sFL3qGYGSaVOJPa190THrhgq6oiiu+O0QLFfEClwuvdRzPy4Xv2RCBUiuTHqPxDH9OhRdBV1RFFeKiVPg2lXmsMVKiGjGfZP3jPByueiwRUVRBiwmo3nlWcaF4XJLE8S+8W7kK7pHADCdKaooSrXiV57K9hvnu1yq0Yeeh1fMdmNM5l70R1tU0BVFcaWYD723wajyzy6/uGDF05B1uYRCoYJjvsrQUS6Kogwk0ppVvoHeW9WrnNPFdZSL5yLR6nJRFKVK8e9yCUZYey2IAdVDsI9pzy3Tbx37Q9pV0BVFcaXoKJfeDjPM90f3ek3R4Cl4WBUZtpjZ6gdr3Zegi8gZIrJCRBpF5CqXPCeLyFsislREXg62moqiVCP9PVO0spN40pOm8i/iz+XSHxZ6pFgGEQkDtwCnAU3AHBF5whizzJZnJHArcIYxZr2IjKtQfRVF6UP8jtQof2JRsYTSCErfjb2sUP6xgb3AxfFAozFmtTGmB3gAODsvz3nAo8aY9QDGmK3BVlNRlEqyuaWL385cjmXlugx+N3OF53l+3Qq3vNjIO1taAXKuUTBs0U9xGxfAG7dCx3aOWf57ABq3ttG0o8NXXfwQTnTz/tC7QKGFvqSpMPpiGnv1c9oS74GZP4LOnYHV0Qk/gj4eeNe235RKs3MosIeIvCQi80Tkq04FichkEZkrInObm5vLq7GiKIFz+QPzuefFRbzVtDOTtmTDLm6btabouSNoc51YVE8PDXTzp5kL+NKfXgVg3vodrmX5WoJu2skw84fw7E+IJDozyd++b0Hxc+3EOpP/HDh8498z2/k+9EvunuNRqCGExTA6ctV98d/hjZvh+Wug0739vcWPoDt9UvnP0QhwHHAWcDrwExE5tOAkY6YZYyYZYyaNHTu25MoqilIZPrnrIRY1fINIW3YxCsuHuTy2ZTELGyZz+I7nHI+/XH8FyxsuZEnDJVxl3QZAZFdT5nhvOg7Xb92Zs98TtyhpRMpv9odf7eV4KGTZQuDmxXLxiuhoDPxM7mBxwyWYRE8mvem9XQB0LJ0O1x0Ajc73q7f4EfQmYIJtf19go0OeGcaYdmPMNuAV4KhgqqgoSqU5ofsNAKLt+T9tb8bsSnal7dfqbB3vJVlr9Gx5BYD6tqygF/hYSojlsrW1K2c/KeV553sJesJ5kYreYTLtNInsQ2FDS1LcB3emHpjr36zAtf0J+hxgoogcKCJR4Bzgibw8jwMfE5GIiAwGTgDeDraqiqJUjvIsZTFJ0TJSdHyFy1UrO/U/iD7K/AiOxTpe+zF6bvFRLsaYuIhcCswEwsCdxpilIjIldXyqMeZtEZkBLAIs4HZjzJJKVlxRlErgf4geQCgl6JaEi5acEUabT7pXY7XzTk0WW1r9y7mQt8sl2wdg7L73PgqS7uuxaoyZDkzPS5uat/9b4LfBVU1RlD5H8ne9BVGs5LqZVgkWupe0lSLw4qMDNYjJPfn3wO+qSDnXlnxnSGWGN+pMUUVRXCnqXjBJQTeh4ha6U6EFC1yUInR5s0orZQTnL2jhdRnJGZFpP69vLHQVdEVRXClmjWZcLpQg6F7XK8GiDuVZ6E5DJ71C3fqlXL9+7qxR/wG+eoMKuqIorsJd3OXi34fuROESdCW4XFJvBxDs0nOFlOJDN87H+8iHroKuKIorfl0uVqi4D91JdAtdLv4Rm/UtGEfNDCRAVpkLWedMklJBVxSlr0iLbamyEypj2KLnNUrQX7uFnjm9QDgDCB9m/PvQkw6a1L3M8fZUYvRNISroiqK44tuH3g8ul3w/dG4Mc8csZVKayyV7lscoF/WhK4rS1xT1oaddLj4EPV2Wl/ehXB+6a8EBCGe5bhv76BhTMGyxMqigK4rie2x1PqH0sMUyZ4r2RnDt49CNS3iw8meeukeELDaxyOl4X80eVUFXFMVG/jR3d/G6d/Y6mne2AfDcyu1sfGchPHwR7FjnmD/bKWofiG7x2IINrHj0lyz91/P87PHCCeZrtrVz/YzlBeltndlYLNlO0WzZc9ZupxK+ar/inNspmnusJ1GZlZlU0BWlr4h1QZd7LO0+obsNetzjhueHih3NLte8P35sCe+1JcPPWibE6w/fAEsewSx/skglcq9x+YNv8b5Fv+Xw6f/Jug0bCnJffOcbPPBSYfAve92GSWEY3C9OfaNgUpAjDm8JknM4V3zriONGjg/ddm3Jc7ksfLcyIXRV0BWlr7jjP+A3+/VvHa4dD789xFfWSPtmXmu4zFdeA7S0J6MfLtvo8tDyMazw2forC/JM7r6L+Q1TCtInhrLif3b49fLdGgvu9TycX8fbo7/zVWyuuydXaiu13qgKuqL0FZsX93cNksTaXQ/ZhaauY0tZxbd2uVuwQMkO5ZMTs3P2LeNcgIjjXNHiF1jziudZ+WUeEVrrXpZtYlGOta7j0BVF6XMqKDyOnZY+LNWCZUcDqU0JlG1N210uKuiKovQxfSM8pYXPzbduLRfZcqy5LzF28qHb11b1UYRTWfZhixrLRVGUasc+bqXY0Mf0UXFM9Y/XGfni78tXXTSP/zrmXs8+sUgtdEVRBggi5Vmc/joH8x8Bbj700qI1+qe8IYb2tpXl2y8DFXRFURyt65ICZdlimFQaV0EvO3xukTeLEtY5rZAnxTcq6IqiZMgRywqpkxQsQefPVZPd93hwVMS1UV58mVwLvW+UXgVdUZQMORNqSjqvlDCJ9mu4xA/PqUd+p6h/0S7bh54TZKs8cq6dfw3tFFUUpS8pRXPEZTunPIcjAoSKSqY/H3r5FIm9UpL4Oot4uX0MpaKCriiKC8VFKBNB0Yel7YhVuoXuJuhOgbj8jVp0yuRhXfvE7r8v1PN+tNBF5AwRWSEijSJylUe+D4lIQkS+EFwVFUWpPL3vFM2cV6Ifu5iF7teH7qy75Y1Dzz1amg/dOdpiZYJx5VNU0EUkDNwCnAkcBpwrIoe55LsOmBl0JRVFqTAZDbJ3WPo/PcflUuTEHNdyGZZ94D704oWUldf+IKimTtHjgUZjzGpjTA/wAHC2Q77vAI8AWwOsn6IoALOnQuNzmd14wuITN7zMz/+5tLzy5v6Ffy7cyCPzmnKS7b7eAjFc+CAsftixOPuwxSPW/dXz0vZr/t/z77iK3WMPTINrxrCXyZUUd5cLrNjSlpM27eVGz7oAdMdyF8q4fdZqXl65zTnzrN97ljXy1asz7Rm++G5Y9oRjvkoN8PQj6OOBd237Tam0DCIyHvgcMNWrIBGZLCJzRWRuc3NzqXVVlN2XGT+Aez+f2X21cRsrt7Txl9fWllfek5fznfsX8L2/L0wlJCUm13rO4x+T4ZGLM7sjaWUQPQCMkx2EUm6FwT3OYpgW4rnrsqFjZy7dzAicg4V9dvn/gBUjmheu1s3WFSue04BhdPDU7OIB0ZZvbs3Zv/mpfzFIum0pNnfJ89d4ljV4zTOEUw/FUfNuhIcugHh34dJ4RWtVHn6WGXFeCCSXPwI/MMYkvGJBGGOmAdMAJk2a1M9D8BVl4FKpH0/OxPUiF3mr4b8y25dF/lH2Nd9suLSk/G4W+rntd2OX4cUNl/gsMLeh9nYB5U4UzfLQ15A9TstJqsyMVn8WehMwwba/L7AxL88k4AERWQt8AbhVRD4bRAUVRekLUgLTx1Mdg/QtH90zv8whjcHFcnFk5dMFZfSnhT4HmCgiBwIbgHOA8+wZjDEHprdF5C7gSWPMY8FVU1GUvqHcKIPlEaQvudzx6cXlvPc3QnyFIOg9RQXdGBMXkUtJjl4JA3caY5aKyJTUcU+/uaIoAwcTwAzJotfosyWTfVLCqJzyr5G7W6lRL76W6jbGTAem56U5Crkx5uu9r5aiKP2Cy7C7YJDU/5UZzlepB0UQdSy8lzr1X1GUCuEYbbECmmNcJt6UVIbrxKLyBL3oTFUrAHdJH/VNqKArimKjssJjTH7clwAt9DIN9OI1CMBCz/ehV+g2q6AripIhx4ceuFVZ6HgINNqtKU8nKzWEMIdqiuWiKMpuQkUFHawAXC6lRHP0Rx90ilZLLBdFUXYfXNfEDAQpcLkEeY2y45YXzZES494oe8GwRbXQFaU26O91yjzIcbmUsPSaXyxjeh0b3O3sci30oi6X1HFjJbzzeZZR/qmloIKuKH2Mv3Uu+4ccd0iF6tnbYYvuwbkqNL49JehWrx7EOspFUWoSqwKWb1DkOlzc61mufz1fFKtjipG/qf9Wryz0/FEu6nJRaoH5dyfDsLrQuLWNnzy2JHjRW/QQzPMO61oSiRg8finsXO94eHNLF1c+vJCeuIOV+9KvYe1r2f0N8+CZn+T8yDt64nzvoYVsb+/JPTfWadsx/DhyD2xamEm5fdZqnl22paSmTH15la1IAytnsuj+n/D7mcsd89/w6/9h8xsPlHSNNJV8lh2eeJuvRJ4v40x/naK9cUG9sLxvooqroCt9yxPfSYZhdWHKX9/k+dnzWL3NOaRq2Tz6DfjnfwdX3tpZsOAezIMXQOeOgsM/fmwJD81t4sUVhT/k8Ku/h7s+mU247VR4/aYcK+7vc5t4ZH4TNzy7MvfkddkHwTh2cknkacxfzsqk3frUm1x+9yxfTRhOOyNo4zdPZ4XbGAvu+xJHrriJoabN8bwreqax9zNTfF3DjiFp2e9J9n6NF5e44x4EbtXbreXOna7ZemOhh9pyH7I76se75OwdKuhKVfHNnjt5veG/iXSW/kPvSza2dAEgm96C6w5wzVeK+Dj51gvGaQ8aldl8OHo1AF22t4D5DVN4of57vq63qOEbLGyYnKpn2gzNHr8j6r2YQzlYBv4YvTWzf3/0VyWXMUZagqxS7md03f6FGVKfS2+GcV5R90jO/vwxny27LC9U0JWq4vj4fABC3Tv7tyJFeK+tu3imErFPMXcVD1v6fqHkIjHxRO6DYE/Z2Zta9OLc/i++HIp60NOjXEwvfOj5VKjzQAVdUaoEK5FdmSctMgW/+4qPkKms4vZupEhlKD7SJt0pWv2BxFTQFaVKsHIs9OTfghXAKiToaYGpxOxQO9Uo6H6fYYEE6cpcUgVdUWoa11d6mwjarfgKVaJyRVcsCnhvKTbKJYhx6PlXrIz0qqArSjlUQPhyfOipvyK510oEaCXaSUttpQW3Ki30Iq3OPIZ6Mw49j0o5zlTQFaVKsA+LS1uFgmAXnISHhR6Iu6TSgluNel6ETKdooA9TdbkoSvUQaNzXJK6CYfObJxysRJM55q2W/jr1KuxDr8ZZsj5juVgB9l+oD11RqoiKrObjINYFLpe4+2t/vJig+6h05TtFqzeOTTEstdAVpUyq0teapRLde7kul/R1yLHQLavQ5ZKWhljCW3ASfu5pRe+7VGlgMn/DFgMN9avj0JXdiqr84duoxA/SZmGnA2MlLXSbyyXhYMWn8hZzuXhrdd88QC2H+lc9FfChV+q56UvQReQMEVkhIo0icpXD8fNFZFHq3+siclTwVVV2KwIcUVAJyl2Q2AsnCz21l83jYIWHU2MmYoneu1wq3ylafW9evicWVbuRgQ9BF5EwcAtwJnAYcK6IHJaXbQ3w78aYI4FfANOCrqiyu1HdP55KuFyM5TBTVCTPQi90uYRSueNFLMhiFnySyt73QKfPB0XRPtFKjHKpDH4s9OOBRmPMamNMD/AAcLY9gzHmdWNMOoTabGDfYKup5NO4tY0rHnyrII5HmhueXcmMJZv7uFbeOIV13dzSxbf+No+OnlyhKmV1mBlLNnHjc+8AcMuLjTyxcKPvc294ZjkvXv8lnnnuadc83fEEl943n/XvdWQTAzDQ2+45D7pasEx6lmb2s3zrtRlcG7mNT6/6OWzNRkPc6+mLefBvtzHlnnmZtBAW5902my/9+Y1M2pk3zuK0P7zMkg3ZQFa/nbmiaJ1ufG5l0Ty9oRKrIPWWI2IL2fLy7Uy94zbH4++98TeO/9Vz3PLCO31cs9LxI+jjgXdt+02pNDcuBhx/HSIyWUTmisjc5uZm/7VUCvjuQ2/xjwUbWLpxl+PxG59/hyn3znM81l984+45BWnXz1jO9MWbeXpx3sOnBGtoyr3zuSElRHfNnM2P73/V+4TmFZlX//temMspHTM5epZ7ONg3Vr3Hk4s28ePHl9jqV2QyirE4SLwfLENXPQUL7iWUXpKtuw12Jn9qv+/+OedGXuSI956GRy7KOe/L73yfbluExRAWr6/axtgd2bjoWzY10dPcyKNz12bS7n5jnXt9U2bq/lJaLPVSMAjS3jdxwUtlzxe/x5R3v+947BTmsLW1mxeXB3NvtpnhgZTjhB9Bd7JFHL/NInIKSUH/gdNxY8w0Y8wkY8yksWPH+q+lUhNcGJ5RkJaOVZI/AqPcIWJzGr7NrPrL3DM0Pge3HA8L/S/QEErVMXcMtbegn97yIC/Uf59hO9/2zNe4tTWzPfLvX4A/fhCABmwLWzj4nSdKU2Y7LIYjZTWP1l+dSZvfMIWX67/Lmet9hsBNXeJndff4y18m+9374YqW3xuazQjP4xKQO6rRVCYWOvgT9CZggm1/X6DA9BCRI4HbgbONMe8FUz2lljgm1FiQFk59AwsmnPSiU3SEdLgfbE65HTYv8l1eOJQSdJuwSpEOsoO7lwHQ0L7BM19za1dmO9S1PbNt2X+aDtfKXxhiuEubD2qzv6X1r7ujAnOxAqWHiOfxoKrfbeq44CSHuOsB4EfQ5wATReRAEYkC5wBP2DOIyH7Ao8AFxpjKOuGUAYtTR2JWLHPTK9Z5VsYoi7QQ5XQqFhF0v52mxpDxobue73it3PIjFA/aVa2hsQYKoYAsdIMwfuSgQMrKx/uRBBhj4iJyKTATCAN3GmOWisiU1PGpwE+B0cCtqVfouDFmUkVqrNQUaZdL/pC6auo8S7tc7FUM6vXbGCvrQ0+T727y8RCK+KhPlRvIVY1gBXb/giupkKKCDmCMmQ5Mz0ubatu+BLgk2Kopfqge2SuOU/yKlIFeOEbaVDhMbAo/P62Q00OniIXuO1aH0wdoxfLyOF0rt/wI3m80xhQLXjuQvkmVweszqyMRqIVeKfeTzhQdoKS1pTrDkfon7Njh6Ly+ZiVIi5yX2KX9/IkSfOh+sZxcLomYD5dLLnVuLpdUMbFE/0cjr/Y3BK/743p/y8CQNRKCRgV9gBMvMjuwmnD6wWRHueQd6KOZomE/rgpx8PMH5kN3yGfFcsTP+eGW70P3vl9xy/tFf+B8i/qHCInMBK7eYghV7OGmgj7AKTY7sNpJd4rmC1vlZ+Ulr5v2X3u9bqerllPHwN6MnHzoeeLsw0KPiLOgp0uOW8Us9D6wn6vdRPegjkRgbzgGh6UFA0IFfYAzkCx0Lx96wbT0clwaZVj1fjo3026tXB96MK/gjs+FRCxP5B3ebPL2i1roRVwuffEtqnY993qoR4gHZqFbaqErbviLz1G9hIIctpjoKZ4n70cZ8uFDT/v3c6IsFItsmPrJSpF2OPrQfXSK5vt03QQ9HbUxnrA82yh90BdTqUUd+oI6iQfYKVq5Mfkq6AOcYjGwqwknQXEcQQJ56umTRKx4njxx9ONDT2u3veO22AMnuw6ld50c/eP57XAQ2/x6u1ro6U5RT5eLUR86xTpFE4EOW1SXi+LIQLLQnSyczLDF/FEu5VhDDos/FJAnoNnFkd1/YI4uF58+finiBnI0jPPb4Wih55brbqEnSSTcWxiUK6GWiZAIbu5BBWVXBX2AE3MQ9EovI1YuTsIRdhpBAmX5w028u3imvIWYfblcHH3oeT9uN4Ev9tbgZKHHu/LyFNYtf2ZovsBnTk3JeMxyd7n4eUvZHfAehx4PzEKv5K/T18SiamTlllaun7GCW84/hvpI2DHPm6vf49lXX+NHDQ8jn/oj/OO/4PRr4c0/wZw7YPJLsM/RtHXH+dRNs2ioC/PYtz9CQ12qPCsBD18IH7kMxh9XtE49cYtv3zef733iUN6/l3NEtV1dMS69bwHXff4I9h6RN/238Xm2TL+W3285mgsP6eA35uv8+j+PYPzQEKtu/hwjaWX0f/4W9jsxc0rCsvjVU8s4asJIPnXkPgB896Fs1L0rHnyLP3zpKGTjfHjtRm7f8yeEIxEujD0Ew/ZkRv3p3P3GOv58wXHMX7+TmX+9llMmCK/tcxHH7DeS22etYVA0zK3nH8uYofWQiCej/330CtjnGNpXvc7Cf/yBX9d9hyWb2jLXPWzv4UTCQsIyDKmP8K812/lzne2rfPUI4uNP4KZVyUBaD89/l+eXb+GGhIEQHDjru1w+v5MFZiIf3vlPvhCexXGhbFSJv8RPZ8K+E/hNx2cyaY/OXcvnbeVP7rmCZ6wPAbC2IZl8+6xVyRlws2/hp7Pa+XNdNmDYfW+u58ePLcYYixvrbuHlxFFcsudKfrjrYkBY1dzOAVc9BcC54TVcW2f77Nq3wh2nwc71vDn6s+xq7YEwHDX/RzBkB3z8J47fhyXrt/DxSN5P/M//lrMr8c6C886PPJ+zv684Ry/dM9bEAVc9xdfDM7gwXFgOwCh2cYhxj8QYFMMs58ig1cK+efFx7ESJBzbDs5J9CQNP0Hdtgg3zuGvmRp7bvA+Lmlr40AGjHLNecuer3B26Bgk1goTgnWeSB9J/H50Ml/6L6Ys2sTYV6/q1xm18/P3jYMtSiA6GZY/DpoVw2cLCC2x9G0ZPhHDyNi7d2MKat+fxv7vaefQrB0H9UBi0R84p/1y4kVdWNvN/LzTy688dkVveK79jz+1zuL5uDqyDbd2HccOz9fxuUgsH73wtmefO0+FnOwmbOIdIE7HEkdw2aw0Anxq3ncTYw3hqwVpODi0lSozpC3q4/t+j1D3wFWjdyF0LTqHVDObChl8D8PuG+wi3rGfzSvjmfRtY1nAHbIYfrj2O5XPfIxaLcKA08cJTjXzpAw3Quil5T5Y9AVetY8g9Z/JhYEzP0dRxBJNCK5hrvY9hm99kpLTylnUIdbKdk0LdHBvKDfMT2fAm54efY4cZSvfOOtbt2JOD6zcBEDU9/K71B3y651dcW39Hwa2/MDITNsOVXZM4MdTEKHax44V3cr7R06I3cFb3rznEFplwcOemTJ5f1N2VSR8rLax+4S8cL4M5MLSJz4Tf4DPhN2AHXJno5KHQyQCssvbh6FAjnwy9mVuhJ78LO9cDcMJ7jyWDZKSZ9TvY+6iCNgBcFvmHY3oxjgvlxuY+L/KCa94Ph5Zwdd3drsfvil5XVh3SvGuNZUKo9sJh73r/l+DtJxkuHZwaXsBya79Ayu33qf9Vxbuz4e9f59dALDwZcA/HeUPopmyEv+2rk3/9uCPm3QVPXg5n/cE9z3ur4NYT4cPfgU/8EoBo67s8V38lj7d/Fv74GAzdE75fQqyyvA60p+p/xP9aMwvzzb6VCzve5Oz6x3miLflQOCW0AKaeh/Xpm7m27kE+H54FwIPxk6mb+lLO6a/V/3dm+9mu86AeeBSWNWTzzG34ZnKjPpXwdupfBgO/yX7B74r+lqnxTzMl8k9ujX+Gb0Vy4re58qu6O12PRcTi6fofep7/SPRnHBhyj1P9VP3/5uyfF3nRNe+Pu/8A0cL0jLh7seIp7+MPXeB93AdzrEP5UKj02Hf3RX/tefwDoXcL0naZQQwXZ4vezqOJj/Ld2Ld4Lvp9Dgn5X1gknx4TJuoylr6/2HH0tzjtrbNY2fA1vhN5rNfltdePY0j31opa6APPhx7Kvud+QNZ73pqPis2q9urEyi8kHVo1/RBwoj1lkazPWmqRzmTaB2LJ0Km0lRoQv7A1YafxzpsWcljqGpFUyNWD04spbF7KSaGlmaxHhVYVnD5UugrSgiBdh4Nkk2ue62NfLlrOjMSHfF/TS8z7krV7fISLe75XkP7x7t9yevdvMvvvWOM5oetmzur+FV/tyS4b8JGuG/lE93V8sfunrteIDhtXkHZ5z7cy22d3X8MjiY9m9jeZwjfXudahrDi0cDGPudahme2bj3yEx2zleJH2y3+u5xomdf2Jw7vuoNHax9e577w/WffV1l6c3H2DY56Xjr0xs/0xlzwf7Lqdpz8zj6O6pnEqU4n/z1pO7Po/PtJ1Iyd2/R9PJE4qOOeXsfPpNnUF6ZsGvy+7E47QQ2EeN67o+Sb/0X09/xObnE38zny4cg18Zz5LD7oYgOGDHKyGgBh4FnrIf5Vzn4Qpy1wc0uwpBkqaApFTXvHzhMLIfV6EikywSY5yyXZ3WcZ4WgCVjOeR7qjzCuW6whRfnXCxdSCnhBZQL30ToCsIOqOjWGgdUpC+Km8xgxVmAlsYxRYzin3Jrt6zgbFgYCw7XK8RrSu0v9422bekTWY0zSbr4nvHGs/e4e05+RdZB/HRMRMhz9B/yzqYSaGV9JgwO6P7MNTnbyD9fWplMOmlOuI492nlY4Ynhb+FobTR4JinMzo6s91Sv7djnjYGM3TYCFoYypD6BiJD9mAz2fPaTWHZ75nh7GIQY8l9K+6MjoJUaHkJ+RdzgA1mDI1mX0Zb2UVLGH1w8u/gURhJfn5Rlz6/IBh4Fno4K+hliZPT9G23YvyMtCh7RInTeQ7jtJ1GL9iuGc8fbJFXn/x7VMkhaunREm4jLgDiLjaEfXJNnDAJn6JQLRgg7uPnZPefxk1hGz3b7fDRddl8RHHCxGzndzv4j2KEMeHCzyD9uSQIk2sieOP0ffI7AScUStbVQlzbnftw8JjJGUre+/yVr9wIi7Mn20j23jjdJ0vcDcp0iW41SI89r9QYdBiIgp731PS6NzmWaka8S/DTFZkU0hc4ulxsJDKupHTUQqvIFObK+SnTQu4Vmc5N9DpzhClEYgB+Nf1YpnahdBIxr4eC0/Qfu9sgTijnIdHpIOgJwojDW276QRAnjDH+4quDy2Qxnw8DSYswIed7J6G8h5779zoaSQV58zkC0+2hY+z3xsFCT4Sd3yQgb5UpR1L1Fx2HniXvy+h7QmHadeE0+cSu+2BboqY0Qfcz/tv74ezgQ3cSR1sh+bFcChaKKDFudm+IpFwkboGiAOLG2cLpzPS+Jq3F/g71Wg6xEj2YMUdBdy/D6etl9/HGieTUocs4W+gSLhSqmElb6ElJ8GtlO32dfVvo6TkIHoLu98GettD9hpN2G3tv2fTF6cFn+RB0V4Mq/btVC91G3pfRK9qg40ebiBfLQeZr6uVycThmUg8Lry90+qP066mJmIRj5nRKLB5L7Wd983YxLJwiXrlJJGkLPYr7g9BJxKDQdTAQBd2PhW7/sTvl9xawwntit+jzXS5OFnrchAt+Q/a6xAljMCVMNirfQk+7Ey0jzu2WMAnx53qLhNMWeu8E3W49GwcL3Yp4CXoRl0v6r1roNkK5PnTf0QbT7hMnN0qOW90+vTuWTnQvz0YingwOFfbh1nEWdIcfh0lkHhS5J6eCLsVzj1mWybHC8zso/aw9WS71KSEfjPuMTTfR6zRZCz1GeECGcSrVTeRk0Xs9FJy+M3a3TYxwzvldToJOxLGzL113ixDGQNjnEEJnl4u/h0Ek5U5MXtvhE5dQ5s0BvH3P0dQqJE6CXkodc8Q24mSh1xekpUkUsdAz9VBBt2GzLgRTxEK33dhYaqhekXgfOV8Ir+h9icJyTMpFE6Rohk0Cy+laJl2NZB2zo1ysHEGvy/thVnKadz3JugzC/b65dX512FwuCTMwLfRSA8Q6PQC8HgpOd8Qu4IZQrsuFQvGJE8oZWJBfdpwQlvFvoTt2iubHd3chHYnS1ffsw+WS7kyPeAi6E27tyxmzJqVZ6Jm3ZFdBT5erLpcseX6tmF8LPZYai2QXdJMOzJQtI2aZrI8rndfpA3Cw0E1KXCNlx8p29qFbDr78TNClWK54GmNyOiXzfeaV9KEPkmRdGsTdQndzuXTbfMGxAepyKRVna7z4QhteZdgFvdPBhx4nnBld4nTVuAmTsEwJnaKF+LXQwzkWulNBuS4kL/94nYfLxTEOv6uFnt22HCxp05tOUaMWeiEhu4Xu/UTO+SDTAY8cfOj2h0I8YZH5mqbzOr7rFoqslfJnO3ZkpvNkRkr6tCRMvEC00yUkq5i20LNrc9p/jA151rJXh2VvGZRytXhZ6G4uhYRtNEN8gLpcSqe0Vjp9Y/LF0D4qxMnlEiPiaJ+EUw/6BCEsU0qnqEPANb9vgVbaQnfvRMx5A3G4AemkurB7p2gpdbRbz45DID0s9GJvE+mQy/3uQxeRM0RkhYg0ishVDsdFRG5KHV8kIscGX9UUeePQfccDd7LQU8RtZeT45D1dLg7HMi4Xj1EenmFXncehe7lc0g+R9Jc2+bqcvX6BoFfQQm/IuFxK96HbSbplgrfQexzGfRej22VUTn/gbKDmiqHdonWa5ZgglAlZbCctcAnCWFYvXS5+hy2mfoueLheTPeYk1mlDJhJKr03r1+XinC9H0J2s/ciggrRM/Yq4XDL170+Xi4iEgVuAM4HDgHNF5LC8bGcCE1P/JgN/CrieWfJcLiUvweYk6LYPzr5NOhyro8vF3YfuNXbcs75OCxmYuKOgp6uZyLwVWKkiTM7EngbJfZOopA89fa2whw/VaTIN5Mp3pVwupUzjTuM0OccJv8Plekfxa9hdLk5T2+OECTlYiGGxMsfjVq5RUGqdfH92xVwuUoLLJZLqkHS04ktwudhHITn9VuvK96Gn18lNj7+vBH7Mj+OBRmPMagAReQA4G1hmy3M2cLdJDhGZLSIjRWRvY4x7UI8yeX3trkw4LsFw/czl1D9zJUeznDfax7OVkRwsmzgjPIdhTvd1x5rs9vbVbLn+Q1zSsZKz6kext2yHp2Ede7E/wLpXU+es5a2fHsPRIYfYLk3/gqtHsNrswzGpWCYjTDZM6I5fTuTm6IV8ue1eDg1t4CLgogZgCaxZOoEIcdZH9ifavd0x8NKwdx6jfvWsXENs8UMcntq8bPsvucz2HdtrxT2eb/K/qPuL+8E+wM2HbqdSLpdyotx1EWV4ei64B8u2+IuPU85DJY0fmbSLY4/DzzthQo72SSTH5VKKD718l0vWQndzuYSIGW+XS5r0KBe/hHCOD2+/N47uXB8uF9dhi6m5MOF+FvTxgD0cWxNwgo8844EcQReRySQtePbbr7xQlIMbsj33Hwyt5ZI9FnLm5umExfDFSGEgqmLs2ZEU0b0lG/NifzYX5HMUcxsHSTbS3ItyPKeYfwGwR3wr58TvY2JoQ8E5B5rkLZsQ2+T6rnRO5CXP6zrRagYxzCVSnlfM52LcHT+NfyZO4pboTUSJsdQ6gFGyyzFanxtuLpdhDRHSw9djRApG5/hlpxnCSGl3PFYfMiV7ciLRBjyG1XNn/AwAbuULjq6M4w8YxaBomP9qvIL/jfyNX8bOZ989BjFuWD2DouHkL8XGB8cPh/ecr+UkaMMbcn/CzWZkZnsbIwryn3jwaPYaXjj6JS3oY0YMIW4Zxvv8ntibfOr7x/FvE8cQfiYpXHfGz+CiSDLe/LOJYzktPD/n3DGDskMlnQsP5Yyzt4zhx7ELWWQdxBP1yfjykdSbRf5D6v5vnEjTjg7GDqtn0z232coQQmIIi5X0u+c9e4Z0ZX/7MQf3aEPU/YG83+ih/PcpR9G6uhOWFh5//97DYCUc6rJWQhD4eVQ4hjwoIw/GmGnGmEnGmEljx471U78Cjj4oG+zo6NAqvrHlGs9X/GL4jQznl8cTH+bCzst5LXF4Jm0i/gWvt8RM2FXMe8MFPVfx0/iFNI86luO7b+Xo7ts4P/Yjfhn9bknl3Dn5ZKYnjs9Juyp2CUd88MjM/u1TTueVxBH5pxZlmxnBlfZId3nUj/Y2In4S+3pB2qhx4/F65bkm/lWuiX+Vn3/po6y+9qzcg+OP46EpJ/HXi45npvUh/r3nj/z8nI/x6g9O5dFvfYS/XZJcqMQe6fC6zyfvw3zrEJZbEzLpjyQ+RmjfSbnljzucRVefDg0jYcQETn3/OJab5Dm3xj/DnnvlBgYD+NLJk4iMPrAgfcpF3wBgzxO+zMmHjuUF62gAVgz/CJCM3PieGcbbtjoBvGAdk9m++tOHc94J+/NI4mPJtsTPyRy7Kf6fBdeUQ05NlpE4hs8fawvadvjnkn+P+jInHJzUifbRR/CF4yZwb+I09piYXeDlFSt5v9IrX33umGSbTzp4NF+cNIGT3zeOLWOzIbbPjyXDKc+13kfkmGz9Mhx0cmZzn9QCNKutvTJp9Xu9L/+MDPd86zQ+f9y+fP3jqS7EPQ7IOT58z4MAGDHhg65l9BYpNl1dRE4CrjbGnJ7a/yGAMeZaW54/Ay8ZY+5P7a8ATvZyuUyaNMnMnTu3vFq3NdPe1Uk01kpdOERrPESzGcl42YZF8incakWpJ0ZXdDRxE6I+tgMxhuGRGLusKK1mMHuEOtjJMMSKM0ZaSJjk6+awOkNLZBRdHe3USYIx0QRbeqIMlR7arTDt4WHsYbXQFh1DvKeTEbQTFUMzI5HoYGKEiZJgUHw7rQzlgFAzXZbQGg9jEaYulPRRdslg6sTQIN10WmEG08PW0Ggm1LXRxiBCVpwh1i56LCFuhOHDRxLr7qK1s4udkTGMi3axrSvEYKuDKD30NIwlFO9gbKSTlug4htcZ2nZsZVBDA++1xwiHDIPDFoMiwtbEMEYOqcdq20pr3bjk+PWeXSSiIwn3tDAqatHz1JU0rHsR69y/s3HcRxk+qI4h0Qjvbu8gbhksYxg3rJ6RoU6Id9O5axuhYePY2dLCnqNH05kQundtoyEaIR6qZ2g0BEPH0dLajrRtYtjYCcS7O4hFhjI4AuxYC3WDYcR4Wlt3YXW1MiLcTVdoCInOncTqRmAZQz09ROrqqB86GravgbpBMHgU3SZMS0+ISM8uwDDctBKJ1EH9MIjUgxUnsWsLHXUjGSbdEB1Ca2cnQ6NhJN5NS/0+DO/ZhESHQueOZMf38PHJ/pJ0J3jnThgyNmkS1g2iJR4BAyMGpyy3WCcg0L0rubhJat5EVyxBS2eMPYfnvbL3dNAag2h9PQnLMDgaYcfOnezoMuw1op5YLI6x4oSjgxnaUIe0boL64ck6haPJRVQ6d4KE6I4MoaUzRn2igx5pYOTQBupaN0C4ji4TgVg7DaP3T163YzutnZ0MGzoi2bbBo6BrV/JeidDa0UVHWytjRo1E4t10dPeQsCwkOojhddDRso36ISPYkWigLpycXTpycLK/oa2zG2IdJCJDGRbqor2rm7ohe9CydR0jIhYN9fUwaCTUD6OzdQfd4SEMb6ijvb2VoeEE0jA8OYihbgiEQrRt38zQocNIRAazo6OHkYPqiCQ6ae3sZvDgoXRZIYbUR+joiVMfCRPOe1WKJyx2NG+gfuhITLgB6Wkl1DCcoXUhiLXT1tXD4PguQsP3gnA9rS3NDBsyDKKDae2KYfV0Q08rdbFWBu81Edq2Juuf/k6Eo8nvy7Cs8NPSlPycGmzWuDGwa0PyO9WLjlERmWeMmeR4zIegR0gG2/w4sAGYA5xnjFlqy3MWcCnwSZLumJuMMcc7FJehV4Ku9A2tm2H2rXDqTx0noyiK0vd4CXrRX6kxJi4ilwIzSS6sdacxZqmITEkdnwpMJynmjSSjCV8YVOWVfmTYXnDaNf1dC0VRfOLL7DLGTCcp2va0qbZtA3w72KopiqIopTDwZooqiqIojqigK4qi1Agq6IqiKDWCCrqiKEqNoIKuKIpSI6igK4qi1Agq6IqiKDVC0ZmiFbuwSDOwrszTxwDlR5mqfrR9A5dabhto+6qB/Y0xjsGw+k3Qe4OIzHWb+loLaPsGLrXcNtD2VTvqclEURakRVNAVRVFqhIEq6NP6uwIVRts3cKnltoG2r6oZkD50RVEUpZCBaqEriqIoeaigK4qi1AgDTtBF5AwRWSEijSJyVX/Xp1REZIKIvCgib4vIUhG5LJU+SkSeFZF3Un/3sJ3zw1R7V4jI6f1Xe/+ISFhEFojIk6n9mmmfiIwUkYdFZHnqczypVtonIlekvpdLROR+EWkYyG0TkTtFZKuILLGlldweETlORBanjt0k0os15CqJMWbA/CO5YtIq4CAgCiwEDuvvepXYhr2BY1Pbw0gu73cYcD1wVSr9KuC61PZhqXbWAwem2h/u73b4aOd3gfuAJ1P7NdM+4K/AJantKDCyFtoHjAfWAINS+w8BXx/IbQP+DTgWWGJLK7k9wL+Ak0iuGP40cGZ/t83p30Cz0I8HGo0xq40xPcADwNn9XKeSMMZsMsbMT223Am+T/CGdTVIoSP39bGr7bOABY0y3MWYNyWX+PNdr7W9EZF/gLOB2W3JNtE9EhpMUiTsAjDE9xpid1Ej7SK5iNii1lvBgYCMDuG3GmFeA7XnJJbVHRPYGhhtj3jBJdb/bdk5VMdAEfTzwrm2/KZU2IBGRA4BjgDeBPY0xmyAp+sC4VLaB2OY/AlcCli2tVtp3ENAM/CXlUrpdRIZQA+0zxmwAfgesBzYBLcaYZ6iBtuVRanvGp7bz06uOgSboTn6rATnuUkSGAo8AlxtjdnlldUir2jaLyKeArcaYeX5PcUir2vaRtGCPBf5kjDkGaCf52u7GgGlfypd8Nkl3wz7AEBH5itcpDmlV2TafuLVnwLRzoAl6EzDBtr8vyVfCAYWI1JEU878ZYx5NJW9JvdqR+rs1lT7Q2vwR4DMispakS+xUEbmX2mlfE9BkjHkztf8wSYGvhfb9B7DGGNNsjIkBjwIfpjbaZqfU9jSltvPTq46BJuhzgIkicqCIRIFzgCf6uU4lkeodvwN42xjzB9uhJ4Cvpba/BjxuSz9HROpF5EBgIskOmqrEGPNDY8y+xpgDSH4+LxhjvkLttG8z8K6IvC+V9HFgGbXRvvXAiSIyOPU9/TjJPp5aaJudktqTcsu0isiJqfvyVds51UV/98qW+g/4JMmRIauAH/V3fcqo/0dJvq4tAt5K/fskMBp4Hngn9XeU7Zwfpdq7girtXXdp68lkR7nUTPuAo4G5qc/wMWCPWmkf8HNgObAEuIfkiI8B2zbgfpL9ATGSlvbF5bQHmJS6J6uAm0nNsq+2fzr1X1EUpUYYaC4XRVEUxQUVdEVRlBpBBV1RFKVGUEFXFEWpEVTQFUVRagQVdEVRlBpBBV1RFKVG+H9PBXe9fpKjzQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "m=[1,2,3,4]\n",
    "plt.plot(range(len(total_scores)),total_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.01,  0.  ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([-0.01,  0.1 ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([0.  , 0.09]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.1 , 0.09]),\n",
       " array([0.  , 0.09]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.1 , 0.09]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([-0.01,  0.1 ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.  , 0.09]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([0.  , 0.09]),\n",
       " array([0.1 , 0.09]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.  , 0.09]),\n",
       " array([0.  , 0.09]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.  , 0.09]),\n",
       " array([-0.01,  0.1 ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([-0.01,  0.1 ]),\n",
       " array([0.1 , 0.09]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.  , 0.09]),\n",
       " array([0.  , 0.09]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.  , 0.09]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.  , 0.09]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([0.1 , 0.09]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.1 , 0.09]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([0.  , 0.09]),\n",
       " array([-0.01,  0.1 ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([-0.01,  0.1 ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.2 , 0.09]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.  , 0.09]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([0.  , 0.09]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([0.3 , 0.19]),\n",
       " array([0.3 , 0.29]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([-0.01,  0.1 ]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.2 , 0.09]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([0.  , 0.09]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([0.  , 0.09]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([0.  , 0.09]),\n",
       " array([0.  , 0.09]),\n",
       " array([0.1 , 0.09]),\n",
       " array([0.09, 0.2 ]),\n",
       " array([0.09, 0.2 ]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.  , 0.09]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.2 , 0.19]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.2 , 0.09]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.  , 0.09]),\n",
       " array([ 0.2 , -0.01]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.  , 0.09]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.1 , 0.09]),\n",
       " array([0.  , 0.09]),\n",
       " array([0.  , 0.09]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([-0.01,  0.1 ]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.  , 0.09]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.2 , 0.09]),\n",
       " array([0.  , 0.09]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.  , 0.09]),\n",
       " array([0.  , 0.09]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([0.  , 0.09]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([0.1 , 0.09]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.1 , 0.09]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.  , 0.09]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.2 , 0.09]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([0.40000001, 0.29      ]),\n",
       " array([0.  , 0.09]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([0.  , 0.09]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.  , 0.09]),\n",
       " array([0.2 , 0.09]),\n",
       " array([0.19, 0.2 ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([0.2 , 0.09]),\n",
       " array([0.  , 0.09]),\n",
       " array([ 0.2 , -0.01]),\n",
       " array([0.  , 0.09]),\n",
       " array([0.3 , 0.19]),\n",
       " array([0.  , 0.09]),\n",
       " array([0.3 , 0.19]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.2 , 0.09]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.  , 0.09]),\n",
       " array([0.09, 0.2 ]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([0.  , 0.09]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.  , 0.09]),\n",
       " array([0.2 , 0.09]),\n",
       " array([0.1 , 0.09]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.  , 0.09]),\n",
       " array([0.  , 0.09]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.  , 0.09]),\n",
       " array([0.1 , 0.09]),\n",
       " array([0.  , 0.09]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.2 , 0.09]),\n",
       " array([-0.01,  0.1 ]),\n",
       " array([0.19, 0.2 ]),\n",
       " array([0.  , 0.09]),\n",
       " array([0.39000001, 0.50000001]),\n",
       " array([0.1 , 0.09]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([0.19, 0.2 ]),\n",
       " array([0.29, 0.3 ]),\n",
       " array([0.19, 0.2 ]),\n",
       " array([0.79000001, 0.80000001]),\n",
       " array([0.2 , 0.19]),\n",
       " array([-0.01,  0.1 ]),\n",
       " array([0.3 , 0.29]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.2 , 0.09]),\n",
       " array([0.3 , 0.19]),\n",
       " array([0.2 , 0.19]),\n",
       " array([0.2 , 0.09]),\n",
       " array([0.60000001, 0.49000001]),\n",
       " array([0.3 , 0.19]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([-0.01,  0.1 ]),\n",
       " array([0.2 , 0.19]),\n",
       " array([0.1 , 0.09]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([0.19, 0.2 ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([-0.01,  0.1 ]),\n",
       " array([0.29      , 0.40000001]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([0.2 , 0.19]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.3 , 0.09]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.70000001, 0.59000001]),\n",
       " array([0.1 , 0.09]),\n",
       " array([0.09, 0.2 ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([-0.01,  0.1 ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.1 , 0.09]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.19, 0.3 ]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([0.1 , 0.09]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.29, 0.3 ]),\n",
       " array([0.69000001, 0.80000001]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([0.40000001, 0.39000001]),\n",
       " array([0.3 , 0.19]),\n",
       " array([0.49000001, 0.60000001]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([0.1 , 0.09]),\n",
       " array([0.1 , 0.19]),\n",
       " array([0.40000001, 0.39000001]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.1 , 0.09]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.59000001, 0.60000001]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([0.60000001, 0.49000001]),\n",
       " array([ 0.2 , -0.01]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.09, 0.2 ]),\n",
       " array([0.  , 0.09]),\n",
       " array([0.1 , 0.09]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.39000001, 0.50000001]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([0.60000001, 0.59000001]),\n",
       " array([-0.01,  0.1 ]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.3       , 0.39000001]),\n",
       " array([-0.01,  0.1 ]),\n",
       " array([1.39000002, 1.50000002]),\n",
       " array([-0.01,  0.1 ]),\n",
       " array([0.3 , 0.19]),\n",
       " array([0.2 , 0.09]),\n",
       " array([0.50000001, 0.49000001]),\n",
       " array([0.19, 0.2 ]),\n",
       " array([0.1 , 0.09]),\n",
       " array([0.  , 0.09]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.18, 0.2 ]),\n",
       " array([0.19, 0.3 ]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.1 , 0.09]),\n",
       " array([0.2 , 0.19]),\n",
       " array([0.29      , 0.40000001]),\n",
       " array([0.2 , 0.19]),\n",
       " array([0.70000001, 0.69000001]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.60000001, 0.59000001]),\n",
       " array([0.50000001, 0.39000001]),\n",
       " array([0.2 , 0.09]),\n",
       " array([-0.01,  0.1 ]),\n",
       " array([0.19, 0.3 ]),\n",
       " array([0.49000001, 0.50000001]),\n",
       " array([0.39000001, 0.40000001]),\n",
       " array([0.3 , 0.19]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([0.40000001, 0.39000001]),\n",
       " array([0.60000001, 0.59000001]),\n",
       " array([0.09, 0.2 ]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.2 , 0.29]),\n",
       " array([0.2 , 0.09]),\n",
       " array([0.1 , 0.09]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([0.1 , 0.19]),\n",
       " array([0.  , 0.09]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.2 , 0.19]),\n",
       " array([0.19, 0.3 ]),\n",
       " array([0.  , 0.09]),\n",
       " array([0.29, 0.3 ]),\n",
       " array([-0.01,  0.1 ]),\n",
       " array([0.49000001, 0.60000001]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.3       , 0.39000001]),\n",
       " array([0.39000001, 0.40000001]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([0.2 , 0.09]),\n",
       " array([0.2 , 0.19]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.09, 0.2 ]),\n",
       " array([0.69000001, 0.70000001]),\n",
       " array([-0.01,  0.1 ]),\n",
       " array([0.1 , 0.09]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([0.2 , 0.19]),\n",
       " array([0.3 , 0.29]),\n",
       " array([0.1 , 0.09]),\n",
       " array([0.2 , 0.09]),\n",
       " array([0.1 , 0.09]),\n",
       " array([0.40000001, 0.49000001]),\n",
       " array([0.80000001, 0.59000001]),\n",
       " array([-0.01,  0.1 ]),\n",
       " array([0.19, 0.3 ]),\n",
       " array([0.3       , 0.39000001]),\n",
       " array([0.3 , 0.29]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([0.2 , 0.09]),\n",
       " array([0.50000001, 0.49000001]),\n",
       " array([0.19, 0.2 ]),\n",
       " array([0.2 , 0.29]),\n",
       " array([0.2 , 0.19]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.39000001, 0.40000001]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.2 , 0.09]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.59000001, 0.60000001]),\n",
       " array([0.2 , 0.09]),\n",
       " array([0.40000001, 0.29      ]),\n",
       " array([0.19, 0.3 ]),\n",
       " array([1.10000002, 1.09000002]),\n",
       " array([0.3 , 0.29]),\n",
       " array([0.60000001, 0.49000001]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([0.  , 0.09]),\n",
       " array([0.3 , 0.19]),\n",
       " array([0.09, 0.2 ]),\n",
       " array([0.40000001, 0.29      ]),\n",
       " array([0.09, 0.2 ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([0.1 , 0.09]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.09, 0.  ]),\n",
       " array([0.19, 0.3 ]),\n",
       " array([0.79000001, 0.80000001]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([0.19, 0.2 ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([0.2 , 0.09]),\n",
       " array([0.19, 0.2 ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([0.19, 0.3 ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([0.19, 0.3 ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([0.09, 0.1 ]),\n",
       " array([0.29, 0.3 ]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.50000001, 0.39000001]),\n",
       " array([-0.01,  0.  ]),\n",
       " array([-0.01,  0.1 ]),\n",
       " array([ 0.1 , -0.01]),\n",
       " array([0.2 , 0.09]),\n",
       " array([-0.01,  0.1 ]),\n",
       " array([0.2 , 0.29]),\n",
       " array([0.1 , 0.09]),\n",
       " array([ 0.  , -0.01]),\n",
       " array([0.3 , 0.29]),\n",
       " array([ 0.  , -0.01])]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
